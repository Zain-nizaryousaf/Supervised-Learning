{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec24f3b8-07cd-4cb7-8c7e-042fc22da655",
   "metadata": {},
   "source": [
    "## 1. Loading and Preprocessing\n",
    "\n",
    "In this section, we will load the breast cancer dataset from the sklearn library and perform necessary preprocessing steps.\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "First, we will import the required libraries and load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181cc2fd-3c6e-45ed-85eb-6ad2450f9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408f48a-9da9-4d29-89bd-fce2291f098c",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "#### Handling Missing Values\n",
    "The breast cancer dataset does not contain any missing values. However, it is essential to always check for missing data in any dataset you work with. If missing values were present, appropriate handling methods, such as imputation, would be necessary to ensure model accuracy.\n",
    "\n",
    "#### Feature Scaling\n",
    "Scaling the features is important, particularly for algorithms sensitive to the scale of the input data, such as Support Vector Machines (SVM) and k-Nearest Neighbors (k-NN). We will use `StandardScaler` to standardize the features, transforming them to have a mean of 0 and a standard deviation of 1. This helps ensure that each feature contributes equally to the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d397a4c-5345-4c12-b90e-ce039df9af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f18aef-0ad6-4147-aea7-d89d4b2b9274",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Scaling\n",
    "Standardizing the data ensures that each feature contributes equally to distance calculations, which is crucial for algorithms like k-Nearest Neighbors (k-NN) and Support Vector Machines (SVM). Without scaling, features with larger ranges could dominate the distance calculations, leading to biased model performance.\n",
    "\n",
    "### Train-Test Split\n",
    "Performing a train-test split is essential to prevent overfitting. By training the model on one portion of the data and evaluating it on unseen data, we can better assess the model's ability to generalize to new instances. This approach helps in obtaining a more accurate estimate of model performance.\n",
    "\n",
    "## 2. Classification Algorithm Implementation\n",
    "\n",
    "### 1. Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear model used for binary classification that estimates the probability that an instance belongs to a particular class. It is well-suited for this dataset due to its linear separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904f2e07-be8e-481c-9b82-810ad0f686d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Implement Logistic Regression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "logistic_accuracy = logistic_model.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790abc3-2778-48bb-acdf-413080992e13",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Classifier\n",
    "\n",
    "Decision Trees are a non-linear model used for both classification and regression tasks. They work by splitting the dataset into subsets based on the value of input features, creating a tree-like structure of decisions. Each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome (class label).\n",
    "\n",
    "**Advantages**:\n",
    "- Intuitive and easy to interpret.\n",
    "- Can capture non-linear relationships in the data.\n",
    "\n",
    "**Suitability**: Decision Trees are suitable for this dataset as they can effectively handle complex interactions between features and are robust to outliers. They can also provide insights into feature importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971da8e6-b789-4774-be07-7a7456de9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Implement Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "dt_accuracy = dt_model.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61740432-5ef5-48de-b45f-47b551b4e919",
   "metadata": {},
   "source": [
    "### 3. Random Forest Classifier\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of their predictions (for classification tasks). It combines the results of many decision trees to improve accuracy and control overfitting.\n",
    "\n",
    "**Advantages**:\n",
    "- Reduces overfitting compared to a single decision tree.\n",
    "- Handles large datasets with higher dimensionality effectively.\n",
    "- Provides insights into feature importance.\n",
    "\n",
    "**Suitability**: Random Forest is well-suited for this dataset as it can capture complex relationships between features and is robust to noise and outliers. Its ensemble nature helps improve predictive performance, making it a strong candidate for classification tasks like breast cancer diagnosis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7555bff-54f7-400f-9c7d-1b24dd964a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Implement Random Forest Classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_accuracy = rf_model.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bae130-22e2-49c4-9ff4-87ea3891961d",
   "metadata": {},
   "source": [
    "### 4. Support Vector Machine (SVM)\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful classification algorithm that aims to find the optimal hyperplane that separates data points of different classes in a high-dimensional space. SVM works by maximizing the margin between the closest points of each class, known as support vectors.\n",
    "\n",
    "**Advantages**:\n",
    "- Effective in high-dimensional spaces.\n",
    "- Works well with both linear and non-linear data when using different kernel functions.\n",
    "\n",
    "**Suitability**: SVM is particularly suitable for this dataset as it can handle complex decision boundaries and is robust to overfitting, especially in high-dimensional spaces. Its ability to model non-linear relationships using kernel trick makes it a strong choice for classification tasks in breast cancer diagnosis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ada13b-d0a2-4cde-90e6-222a385b1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Implement Support Vector Machine\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_accuracy = svm_model.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60222a57-2294-47dc-814b-e89b0d032861",
   "metadata": {},
   "source": [
    "### 5. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "k-Nearest Neighbors (k-NN) is a simple, instance-based learning algorithm used for classification and regression. It classifies a data point based on the majority class among its k nearest neighbors in the feature space.\n",
    "\n",
    "**Advantages**:\n",
    "- Easy to implement and understand.\n",
    "- Naturally adapts to multi-class classification.\n",
    "\n",
    "**Suitability**: k-NN is suitable for this dataset as it can effectively classify instances based on local patterns in the data. However, it may be sensitive to the scale of features and computationally intensive with large datasets. Despite this, it can provide valuable insights, especially when the dataset is well-prepared and normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d346b017-2b35-4b4c-a842-5fee78d2daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Implement k-NN\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_accuracy = knn_model.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b796d8-6594-4cad-bd14-3cf3bdc1736c",
   "metadata": {},
   "source": [
    "##  Model Comparison\n",
    "\n",
    "In this section, we will compare the performance of the five classification algorithms implemented on the breast cancer dataset. The models we will evaluate are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. Random Forest Classifier\n",
    "4. Support Vector Machine (SVM)\n",
    "5. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "### Evaluation Metric\n",
    "We will use accuracy as the primary metric to evaluate and compare the models' performance.\n",
    "\n",
    "### Model Performance\n",
    "After training the models and evaluating their accuracy on the test dataset, the results are summarized below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237c85d1-9797-4915-8a87-b4cf4d11f891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracies:\n",
      "Logistic Regression: 0.9737\n",
      "Decision Tree: 0.9386\n",
      "Random Forest: 0.9649\n",
      "SVM: 0.9825\n",
      "k-NN: 0.9474\n",
      "\n",
      "Best Model: SVM with accuracy: 0.9825\n",
      "Worst Model: Decision Tree with accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# Comparing the accuracy of the models\n",
    "accuracies = {\n",
    "    'Logistic Regression': logistic_accuracy,\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'Random Forest': rf_accuracy,\n",
    "    'SVM': svm_accuracy,\n",
    "    'k-NN': knn_accuracy\n",
    "}\n",
    "\n",
    "best_model = max(accuracies, key=accuracies.get)\n",
    "worst_model = min(accuracies, key=accuracies.get)\n",
    "\n",
    "print(\"Model Accuracies:\")\n",
    "for model, acc in accuracies.items():\n",
    "    print(f\"{model}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model} with accuracy: {accuracies[best_model]:.4f}\")\n",
    "print(f\"Worst Model: {worst_model} with accuracy: {accuracies[worst_model]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044d6ee-85d3-4f9a-a936-706e96c97e1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
